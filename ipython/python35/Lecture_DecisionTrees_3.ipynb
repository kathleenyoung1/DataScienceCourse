{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifiers\n",
    "<p>This Notebook gives an overview of DecisionTrees with scikit learn. In it are examples of:\n",
    "\n",
    "<ul>\n",
    "<li>Build a basic decision tree</li>\n",
    "<li>Plotting the decision surface of a decision tree</li>\n",
    "<li>Visualing the decision tree</li>\n",
    "<li>Overfitting (as viewed by decision surfaces) of decision trees with no limits on depth and leaf size</li>\n",
    "<li>Searching for optimal parameters of a decision tree</li>\n",
    "<li>Using decision tree output to assess feature importance</li>\n",
    "</ul>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Tree on Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#These all need to be installed to both run and visualize a tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "\n",
    "from io import StringIO\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here I generate some random data $X=<X1,X2>$ with $Y \\in [0,1]$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Random data example \n",
    "'''\n",
    "n = 200\n",
    "\n",
    "\n",
    "#First Generate the data\n",
    "\n",
    "m = [[0.25, 0.75], [0.75, 0.75], [0.75, 0.25], [0.25, 0.25]]\n",
    "s = .1\n",
    "dat = pd.DataFrame(columns = ['x1','x2', 'y'])\n",
    "for i in range(4):\n",
    "    X = pd.DataFrame(np.random.multivariate_normal([m[i][0], m[i][1]], [[s,0],[0,s]],n), columns=['x1','x2'])\n",
    "    if i == 3:\n",
    "        X['y'] = np.ones(n)\n",
    "    else:\n",
    "        X['y'] = np.zeros(n)\n",
    "    dat = dat.append(X, ignore_index = True)\n",
    "\n",
    "plt.plot(dat['x1'][(dat['y']==1)], dat['x2'][(dat['y']==1)],'r.')\n",
    "plt.plot(dat['x1'][(dat['y']==0)], dat['x2'][(dat['y']==0)],'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Now lets build a tree on our sample data.<br><br>\n",
    "\n",
    "Documentation can be found here:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "<br>\n",
    "Also, here is another useful example:\n",
    "http://nbviewer.ipython.org/github/sujitpal/statlearning-notebooks/blob/master/src/chapter8.ipynb\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want Entropy as our splitting criteria\n",
    "clf = DecisionTreeClassifier(criterion='entropy', min_samples_leaf = 1, max_depth = 3)\n",
    "clf = clf.fit(dat.drop('y',1), dat.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The decision surface can be derived by using the predict function and not having to parse the DT rules.<br><br>\n",
    "This code was adapted from http://scikit-learn.org/0.11/auto_examples/tree/plot_iris.html\n",
    "\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dat.drop('y',1)\n",
    "y = dat.y\n",
    "\n",
    "plot_step = 0.02\n",
    "\n",
    "x_min, x_max = X['x1'].min(), X['x1'].max()\n",
    "y_min, y_max = X['x2'].min(), X['x2'].max()\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "\n",
    "plt.subplot(111)\n",
    "\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.cool)\n",
    "plt.plot(dat['x1'][(dat['y']==1)],dat['x2'][(dat['y']==1)],'r.')\n",
    "plt.plot(dat['x1'][(dat['y']==0)],dat['x2'][(dat['y']==0)],'b.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above is a pretty simple tree, which we can see in the above plot.<br><br>\n",
    "\n",
    "As we allow for more complex trees, visualing the tree becomes very cumbersome, but we can still look at the decision surface. \n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treeComplex(X, y, leaf, dep, i):\n",
    "    clf = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf = leaf, max_depth = dep)\n",
    "    clf = clf.fit(X, y)\n",
    "    plot_step = 0.02\n",
    "    x_min, x_max = X['x1'].min(), X['x1'].max()\n",
    "    y_min, y_max = X['x2'].min(), X['x2'].max()\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax = fig.add_subplot(3, 3, i)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap = plt.cm.cool)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    plt.title('Dep={},Leaf={}'.format(dep, leaf))\n",
    "              \n",
    "leafs=[100, 10, 1]\n",
    "deps=[3, 10, 50]\n",
    "\n",
    "i=1\n",
    "fig=plt.figure()\n",
    "\n",
    "for l in leafs:\n",
    "    for d in deps:\n",
    "        treeComplex(dat.drop('y',1), dat.y, l, d, i)\n",
    "        i+=1\n",
    "        \n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we let the trees grow without any restrictions, we notice that the decision surface gets very fragmented. This defies our intuition that nearest neighbors of a point should have the same class assignment. Since we know that this data was generated from a few simple normal mixtures, we should understand that these more complex surfaces are terribly over fit.\n",
    "<br><br>\n",
    "Beyond toy examples, let's see this on real data.\n",
    "</p>\n",
    "\n",
    "### Decision Tree on Real Data\n",
    "<p>We don't just want to build a tree, we want to optimize the configuration of the tree. We'll use AUC as a metric on our test set and vary the max_depth and min_leaf_size. <br><br>\n",
    "\n",
    "The underlying data has a very rare outcome (less than 5%) so we'll down sample the data first to reach a 50/50 split.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import sys\n",
    "#Get the datadir path set up - note this may only work in Linux/MAC\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "datadir = '/'.join(cwd.split('/')[0:-1]) + '/data/'\n",
    "#If it doesn't work, hard code the datadir\n",
    "#datadir = ${path}\n",
    "import imp\n",
    "import course_utils as bd\n",
    "imp.reload(bd)\n",
    "\n",
    "#Load data and downsample for a 50/50 split, then split into a train/test\n",
    "f = datadir + 'ads_dataset_cut.txt'\n",
    "\n",
    "train_split = 0.5\n",
    "tdat = pd.read_csv(f, header = 0, sep = '\\t')\n",
    "\n",
    "moddat = bd.downSample(tdat, 'y_buy', 9)\n",
    "moddat = moddat.sample(frac=1).reset_index(drop=True) #shuffle rows\n",
    "\n",
    "#We know the dataset is sorted randomly so we can just split by index\n",
    "train = moddat[:int(np.floor(moddat.shape[0] * train_split))]\n",
    "test = moddat[int(np.floor(moddat.shape[0] * train_split)):]\n",
    "\n",
    "def testTrees(X_train, y_train, X_test, y_test, dep, leaf, auc):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', min_samples_leaf = leaf, max_depth = dep)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    if (auc==0):\n",
    "        cm = confusion_matrix(clf.predict(X_test), y_test)\n",
    "        return (cm[0][0] + cm[1][1]) / float(sum(cm))\n",
    "    else:\n",
    "        return roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = 'y_buy'\n",
    "\n",
    "depths = [4, 5, 10, 20]\n",
    "leaves = np.arange(1, 101)\n",
    "\n",
    "#Run all of the options\n",
    "run=1\n",
    "if (run == 1):\n",
    "    #Initialize dictionary of results\n",
    "    res = dict()\n",
    "    for d in depths:\n",
    "        res[d] = list()\n",
    "\n",
    "    #Now train and get results for each option\n",
    "    for d in depths:\n",
    "        for l in leaves:\n",
    "            res[d].append(testTrees(train.drop(lab, 1), train[lab], test.drop(lab, 1), test[lab], d, l, 1))\n",
    "\n",
    "\n",
    "#Now plot            \n",
    "fig = plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "plt.plot(leaves,res[depths[0]], 'b-', label = 'Depth={}'.format(depths[0]))\n",
    "plt.plot(leaves,res[depths[1]], 'r-', label = 'Depth={}'.format(depths[1]))\n",
    "plt.plot(leaves,res[depths[2]], 'y-', label = 'Depth={}'.format(depths[2]))\n",
    "plt.plot(leaves,res[depths[3]], 'g-', label = 'Depth={}'.format(depths[3]))\n",
    "plt.legend(loc = 4)\n",
    "ax.set_xlabel('Min Leaf Size')\n",
    "ax.set_ylabel('Test Set AUC')\n",
    "plt.title('Holdout AUC by Hyperparameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above plot shows how holdout AUC varies by parameters. As we generally expect with most hyper-parameter searches, there is a \"sweet spot\" between letting the model be flexible and over fitting. Here we see that as long as we restrict the min_leaf_size, the max_depth doesn't affect the optimal choice too much.<br><br>\n",
    "\n",
    "We might want to peek at the tree to determine which features it thinks are important. The DT class objects actually have an attribute that gives each feature a score for how important it is in building the tree. \n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The tree gives us a useful tool for analying feature importance\n",
    "'''\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf = 100, max_depth = 20)\n",
    "clf = clf.fit(train.drop(lab, 1), train[lab])\n",
    "\n",
    "\n",
    "#Sort feature names by their importance so we can have a plot sorted by feature importance\n",
    "feat_sorted = [x for _,x in sorted(zip(clf.feature_importances_,train.drop(lab,1).columns.values), reverse = True)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = 0.35\n",
    "#ax.bar(train.drop(lab,1).columns.values, clf.feature_importances_, width, color='r')\n",
    "ax.bar(np.arange(13), sorted(clf.feature_importances_, reverse=True), width, color = 'r')\n",
    "ax.set_xticks(np.arange(len(clf.feature_importances_)))\n",
    "ax.set_xticklabels(feat_sorted, rotation = 90)\n",
    "plt.title('Feature Importance from DT')\n",
    "ax.set_ylabel('Normalized Gini Importance')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
